# ğŸ¡ Home Sales Analysis with PySpark

## ğŸ“š Overview

This project leverages **Apache Spark and SparkSQL** to perform data analysis on a large-scale home sales dataset. You'll use **PySpark** to read, query, cache, partition, and optimize data transformations â€” helping simulate real-world big data workflows.

> **Challenge**: Analyze housing data using SparkSQL to extract key metrics, test query runtimes with caching/partitioning, and manage temporary tables in memory.

---

## ğŸ—ƒï¸ Files

- `Home_Sales.ipynb` â€” Main Jupyter notebook with PySpark code and analysis
- `home_sales_revised.csv` â€” Cleaned housing dataset (provided via AWS S3 bucket)
- `README.md` â€” This documentation

---

## ğŸ”§ Technologies Used

- Python 3.x
- PySpark / SparkSQL
- Jupyter Notebook
- Amazon S3 (data source)

---

## ğŸ“ˆ Key Objectives & Tasks

### âœ… Data Processing
- Load CSV data into a PySpark DataFrame from AWS S3
- Create a temporary view (`home_sales`)
- Perform multiple SparkSQL queries to calculate averages based on different filters

### ğŸ“Š Analytical Queries
1. Average price for 4-bedroom homes by year
2. Average price of homes with 3 beds & 3 baths by year built
3. Average price of large homes (â‰¥ 2000 sq ft, 3 beds, 3 baths, 2 floors) by year
4. Average price by "view" rating for homes priced â‰¥ $350,000

### ğŸš€ Performance Optimization
- Cache the `home_sales` table and compare runtime improvements
- Partition the data by `date_built` and write to Parquet
- Use the Parquet table to rerun high-cost queries

### ğŸ“Œ Memory Management
- Verify if tables are cached or uncached using PySpark
- Uncache the table and confirm status

---

## ğŸ Setup Instructions

1. Clone the repo:
   ```bash
   git clone https://github.com/your-username/Home_Sales.git
   cd Home_Sales
